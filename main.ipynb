{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05431099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/projectnb/cs585/bargav25/project/PoseFormerV2-main\n"
     ]
    }
   ],
   "source": [
    "%cd /projectnb/cs585/bargav25/project/PoseFormerV2-main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93f32c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from common.arguments import parse_args\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import sys\n",
    "import errno\n",
    "import math\n",
    "import logging\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "from copy import deepcopy\n",
    "\n",
    "from common.camera import *\n",
    "import collections\n",
    "\n",
    "from common.loss import *\n",
    "from common.generators import ChunkedGenerator, UnchunkedGenerator\n",
    "from time import time\n",
    "from common.utils import *\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c7f0ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.h36m_dataset import Human36mDataset\n",
    "\n",
    "dataset_path = 'data/data_3d_h36m.npz'\n",
    "dataset = Human36mDataset(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82957ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for subject in dataset.subjects():\n",
    "    for action in dataset[subject].keys():\n",
    "        anim = dataset[subject][action]\n",
    "\n",
    "        if 'positions' in anim:\n",
    "            positions_3d = []\n",
    "            for cam in anim['cameras']:\n",
    "                pos_3d = world_to_camera(anim['positions'], R=cam['orientation'], t=cam['translation'])\n",
    "                pos_3d[:, 1:] -= pos_3d[:, :1] # Remove global offset, but keep trajectory in first position\n",
    "                positions_3d.append(pos_3d)\n",
    "            anim['positions_3d'] = positions_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f6b20a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NpzFile 'data/data_2d_h36m_cpn_ft_h36m_dbb.npz' with keys: positions_2d, metadata"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keypoints = np.load('data/data_2d_h36m_cpn_ft_h36m_dbb.npz', allow_pickle=True)\n",
    "keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e82e3f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "keypoints_metadata = keypoints['metadata'].item()\n",
    "keypoints_symmetry = keypoints_metadata['keypoints_symmetry']\n",
    "kps_left, kps_right = list(keypoints_symmetry[0]), list(keypoints_symmetry[1])\n",
    "joints_left, joints_right = list(dataset.skeleton().joints_left()), list(dataset.skeleton().joints_right())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47368ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "keypoints = keypoints['positions_2d'].item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6318d7d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1384, 17, 2), (1387, 17, 2), (1387, 17, 2), (1384, 17, 2)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.shape for x in keypoints['S1']['Directions 1']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aedd59af",
   "metadata": {},
   "outputs": [],
   "source": [
    "for subject in dataset.subjects():\n",
    "    assert subject in keypoints, 'Subject {} is missing from the 2D detections dataset'.format(subject)\n",
    "    for action in dataset[subject].keys():\n",
    "        assert action in keypoints[subject], 'Action {} of subject {} is missing from the 2D detections dataset'.format(action, subject)\n",
    "        if 'positions_3d' not in dataset[subject][action]:\n",
    "            continue\n",
    "\n",
    "        for cam_idx in range(len(keypoints[subject][action])):\n",
    "\n",
    "            # We check for >= instead of == because some videos in H3.6M contain extra frames\n",
    "            mocap_length = dataset[subject][action]['positions_3d'][cam_idx].shape[0]\n",
    "            assert keypoints[subject][action][cam_idx].shape[0] >= mocap_length\n",
    "\n",
    "            if keypoints[subject][action][cam_idx].shape[0] > mocap_length:\n",
    "                # Shorten sequence\n",
    "                keypoints[subject][action][cam_idx] = keypoints[subject][action][cam_idx][:mocap_length]\n",
    "\n",
    "        assert len(keypoints[subject][action]) == len(dataset[subject][action]['positions_3d'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "905749b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('disk I/O error')).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "for subject in keypoints.keys():\n",
    "    for action in keypoints[subject]:\n",
    "        for cam_idx, kps in enumerate(keypoints[subject][action]):\n",
    "            # Normalize camera frame\n",
    "            cam = dataset.cameras()[subject][cam_idx]\n",
    "            kps[..., :2] = normalize_screen_coordinates(kps[..., :2], w=cam['res_w'], h=cam['res_h'])\n",
    "            keypoints[subject][action][cam_idx] = kps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dddcaaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects_train = 'S1,S5,S6,S7,S8'.split(',')\n",
    "subjects_test = 'S9,S11'.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2ae76e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch(subjects, parse_3d_poses=True):\n",
    "    out_poses_3d = []\n",
    "    out_poses_2d = []\n",
    "    out_camera_params = []\n",
    "    for subject in subjects:\n",
    "        for action in keypoints[subject].keys():\n",
    "\n",
    "            poses_2d = keypoints[subject][action]\n",
    "\n",
    "            for i in range(len(poses_2d)): # Iterate across cameras\n",
    "                out_poses_2d.append(poses_2d[i])\n",
    "\n",
    "            if subject in dataset.cameras():\n",
    "                cams = dataset.cameras()[subject]\n",
    "                assert len(cams) == len(poses_2d), 'Camera count mismatch'\n",
    "                for cam in cams:\n",
    "                    if 'intrinsic' in cam:\n",
    "                        out_camera_params.append(cam['intrinsic'])\n",
    "\n",
    "            if parse_3d_poses and 'positions_3d' in dataset[subject][action]:\n",
    "                poses_3d = dataset[subject][action]['positions_3d']\n",
    "                assert len(poses_3d) == len(poses_2d), 'Camera count mismatch'\n",
    "                for i in range(len(poses_3d)): # Iterate across cameras\n",
    "                    out_poses_3d.append(poses_3d[i])\n",
    "\n",
    "    if len(out_camera_params) == 0:\n",
    "        out_camera_params = None\n",
    "    if len(out_poses_3d) == 0:\n",
    "        out_poses_3d = None\n",
    "\n",
    "\n",
    "    return out_camera_params, out_poses_3d, out_poses_2d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e5ab1f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "cameras_train, poses_train, poses_train_2d = fetch(subjects_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "41d21d72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600, 600, 600)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cameras_train), len(poses_train), len(poses_train_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4478a350",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "num_input_frames = 81\n",
    "\n",
    "receptive_field = num_input_frames \n",
    "\n",
    "pad = (receptive_field -1) // 2 # Padding on each side of the input vid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ff4e3543",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = ChunkedGenerator(batch_size, None, poses_train, poses_train_2d, 1,\n",
    "                                    pad=pad, causal_shift=0, shuffle=True, augment=True,\n",
    "                                    kps_left=kps_left, kps_right=kps_right, joints_left=joints_left, joints_right=joints_right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2fa20b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 1, 17, 3)\n",
      "(512, 81, 17, 2)\n"
     ]
    }
   ],
   "source": [
    "for _, batch_3d, batch_2d in train_generator.next_epoch():\n",
    "    print(batch_3d.shape)\n",
    "    print(batch_2d.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fd38805a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cameras_valid, poses_valid, poses_valid_2d = fetch(subjects_test)\n",
    "\n",
    "test_generator = UnchunkedGenerator(None, poses_valid, poses_valid_2d,\n",
    "                                    pad=pad, causal_shift=0, augment=False,\n",
    "                                    kps_left=kps_left, kps_right=kps_right, joints_left=joints_left, joints_right=joints_right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ccaf67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e3b8f5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_data_prepare(receptive_field, inputs_2d, inputs_3d):\n",
    "    inputs_2d_p = torch.squeeze(inputs_2d)\n",
    "    inputs_3d_p = inputs_3d.permute(1,0,2,3)\n",
    "    out_num = inputs_2d_p.shape[0] - receptive_field + 1\n",
    "    eval_input_2d = torch.empty(out_num, receptive_field, inputs_2d_p.shape[1], inputs_2d_p.shape[2])\n",
    "    for i in range(out_num):\n",
    "        eval_input_2d[i,:,:,:] = inputs_2d_p[i:i+receptive_field, :, :]\n",
    "    return eval_input_2d, inputs_3d_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "16c29b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplePoseNet(nn.Module):\n",
    "    def __init__(self, num_joints, num_frames, input_dim, output_dim):\n",
    "        super(SimplePoseNet, self).__init__()\n",
    "        self.num_joints = num_joints\n",
    "        self.output_dim = output_dim\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(num_frames * num_joints * input_dim, 1024)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(1024, num_joints * output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = x.view(-1, 1, self.num_joints, self.output_dim)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "11cb8746",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class LSTM_PoseNet(nn.Module):\n",
    "    def __init__(self, num_joints, num_frames, input_dim, output_dim, hidden_dim=512, num_layers=1):\n",
    "        super(LSTM_PoseNet, self).__init__()\n",
    "        self.num_joints = num_joints\n",
    "        self.num_frames = num_frames\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        # LSTM layer that will process the entire sequence\n",
    "        self.lstm = nn.LSTM(input_size=num_joints * input_dim,\n",
    "                            hidden_size=hidden_dim,\n",
    "                            num_layers=num_layers,\n",
    "                            batch_first=True)\n",
    "\n",
    "        # Output layer that maps from hidden state space to the output space\n",
    "        self.fc = nn.Linear(hidden_dim, num_joints * output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Reshape input to match LSTM input shape: (batch, seq_len, features)\n",
    "        x = x.view(-1, self.num_frames, self.num_joints * self.input_dim)\n",
    "        \n",
    "        # LSTM output: (batch, seq_len, hidden_size)\n",
    "        lstm_out, (hn, cn) = self.lstm(x)\n",
    "        \n",
    "        # We use the last hidden state to predict the output\n",
    "        # lstm_out[:, -1, :] - takes the last timestep's hidden state\n",
    "        x = self.fc(lstm_out[:, -1, :])\n",
    "\n",
    "        # Reshape to (batch_size, 1, num_joints, output_dim) for consistency with your previous model's output shape\n",
    "        x = x.view(-1, 1, self.num_joints, self.output_dim)\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2ea43098",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=500):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        self.pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.pe = self.pe.unsqueeze(0)  # No transpose needed\n",
    "        self.register_buffer('permanent_pe', self.pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply positional encoding to the second dimension (sequence dimension)\n",
    "        return x + self.permanent_pe[:, :x.size(1), :]\n",
    "\n",
    "\n",
    "class SimpleTransformerPoseNet(nn.Module):\n",
    "    def __init__(self, num_joints, num_frames, input_dim, output_dim, d_model=256, nhead=8, num_encoder_layers=3):\n",
    "        super(SimpleTransformerPoseNet, self).__init__()\n",
    "        self.num_joints = num_joints\n",
    "        self.output_dim = output_dim\n",
    "        self.input_linear = nn.Linear(num_joints * input_dim, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, max_len=num_frames)\n",
    "        transformer_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=512, dropout=0.1)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(transformer_layer, num_layers=num_encoder_layers)\n",
    "        self.output_linear = nn.Linear(d_model, num_joints * output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), x.size(1), -1)  # Reshape input to (batch, seq_len, num_joints * input_dim)\n",
    "        x = self.input_linear(x)\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.mean(dim=1)  # Aggregate over sequence and project to output dimension\n",
    "        x = self.output_linear(x)\n",
    "        x = x.view(x.size(0), 1, self.num_joints, self.output_dim)  # Reshape to desired output format\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "80c1fbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fe571a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr4/cs640/bargav25/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "min_loss = 100000\n",
    "width = cam['res_w']\n",
    "height = cam['res_h']\n",
    "num_joints = keypoints_metadata['num_joints']\n",
    "\n",
    "\n",
    "input_dim = 2\n",
    "d_model = 48\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "dropout = 0.1\n",
    "seq_length = 81\n",
    "\n",
    "# model_pos = SpatioTemporalTransformer(num_joints=num_joints, input_dim=input_dim, d_model=d_model, \n",
    "#                                   num_heads=num_heads, num_layers=num_layers, dropout=dropout)\n",
    "# model_pos = SimplePoseNet(num_frames=receptive_field, num_joints=num_joints, input_dim=2, output_dim = 3)\n",
    "\n",
    "model_pos = LSTM_PoseNet(num_joints, num_frames=receptive_field, input_dim=2, output_dim=3)\n",
    "\n",
    "# model_pos = SimpleTransformerPoseNet(num_joints, num_frames=receptive_field, input_dim=2, output_dim=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "eb6a5c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "40eb0406",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pos = model_pos.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "089e1f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model_pos.parameters(), lr=0.001)\n",
    "criterion = mpjpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0bb5a2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch = 15\n",
    "no_eval = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c244485d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6093it [09:16, 10.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] time 10.69 lr 0.000100 3d_train 61.820666 3d_valid 99.947886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6093it [09:16, 10.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2] time 10.71 lr 0.000099 3d_train 46.895979 3d_valid 252.961015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "241it [00:23, 10.95it/s]"
     ]
    }
   ],
   "source": [
    "lr = 0.0001\n",
    "lr_decay = 0.99\n",
    "losses_3d_train = []\n",
    "losses_3d_train_eval = []\n",
    "losses_3d_valid = []\n",
    "\n",
    "initial_momentum = 0.1\n",
    "final_momentum = 0.001\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    start_time = time()\n",
    "    epoch_loss_3d_train = 0\n",
    "    epoch_loss_traj_train = 0\n",
    "    epoch_loss_2d_train_unlabeled = 0\n",
    "    N = 0\n",
    "    N_semi = 0\n",
    "    model_pos.train()\n",
    "\n",
    "    for _, batch_3d, batch_2d in tqdm(train_generator.next_epoch()):\n",
    "        inputs_3d = torch.from_numpy(batch_3d.astype('float32')) # [512, 1, 17, 3]\n",
    "        inputs_2d = torch.from_numpy(batch_2d.astype('float32')) # [512, 3, 17, 2]\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            inputs_3d = inputs_3d.cuda()\n",
    "            inputs_2d = inputs_2d.cuda()\n",
    "        inputs_3d[:, :, 0] = 0\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predicted_3d_pos = model_pos(inputs_2d)\n",
    "\n",
    "        # print(predicted_3d_pos.size())\n",
    "        # print(inputs_3d.size())\n",
    "\n",
    "        loss_3d_pos = criterion(predicted_3d_pos, inputs_3d)\n",
    "        epoch_loss_3d_train += inputs_3d.shape[0] * inputs_3d.shape[1] * loss_3d_pos.item()\n",
    "\n",
    "        N += inputs_3d.shape[0] * inputs_3d.shape[1]\n",
    "\n",
    "        loss_total = loss_3d_pos\n",
    "\n",
    "        loss_total.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    "        del inputs_2d, inputs_3d, loss_3d_pos, predicted_3d_pos\n",
    "\n",
    "    losses_3d_train.append(epoch_loss_3d_train / N)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model_pos.load_state_dict(model_pos.state_dict(), strict=False)\n",
    "        model_pos.eval()\n",
    "\n",
    "        epoch_loss_3d_valid = 0\n",
    "        N = 0\n",
    "        if not no_eval:\n",
    "            # Evaluate on test set\n",
    "            for _, batch, batch_2d in test_generator.next_epoch():\n",
    "                inputs_3d = torch.from_numpy(batch.astype('float32')) # [1, 2356, 17, 3]\n",
    "                inputs_2d = torch.from_numpy(batch_2d.astype('float32')) # [1, 2358, 17, 2]\n",
    "\n",
    "                ##### apply test-time-augmentation (following Videopose3d)\n",
    "                inputs_2d_flip = inputs_2d.clone()\n",
    "                inputs_2d_flip[:, :, :, 0] *= -1\n",
    "                inputs_2d_flip[:, :, kps_left + kps_right, :] = inputs_2d_flip[:, :, kps_right + kps_left, :]\n",
    "\n",
    "                ##### convert size\n",
    "                inputs_2d, inputs_3d = eval_data_prepare(receptive_field, inputs_2d, inputs_3d) # [2356, 3, 17, 2] \n",
    "                inputs_2d_flip, _ = eval_data_prepare(receptive_field, inputs_2d_flip, inputs_3d)\n",
    "        \n",
    "                if torch.cuda.is_available():\n",
    "                    inputs_2d = inputs_2d.cuda()\n",
    "                    inputs_2d_flip = inputs_2d_flip.cuda()\n",
    "                    inputs_3d = inputs_3d.cuda()\n",
    "\n",
    "                inputs_3d[:, :, 0] = 0\n",
    "                \n",
    "\n",
    "                predicted_3d_pos = model_pos(inputs_2d)\n",
    "                predicted_3d_pos_flip = model_pos(inputs_2d_flip)\n",
    "                predicted_3d_pos_flip[:, :, :, 0] *= -1\n",
    "                predicted_3d_pos_flip[:, :, joints_left + joints_right] = predicted_3d_pos_flip[:, :,\n",
    "                                                                            joints_right + joints_left]\n",
    "\n",
    "                predicted_3d_pos = torch.mean(torch.cat((predicted_3d_pos, predicted_3d_pos_flip), dim=1), dim=1,\n",
    "                                                keepdim=True)\n",
    "\n",
    "                loss_3d_pos = mpjpe(predicted_3d_pos, inputs_3d)\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "                epoch_loss_3d_valid += inputs_3d.shape[0] * inputs_3d.shape[1] * loss_3d_pos.item()\n",
    "                N += inputs_3d.shape[0] * inputs_3d.shape[1]\n",
    "\n",
    "                del inputs_2d, inputs_2d_flip, inputs_3d, loss_3d_pos, predicted_3d_pos, predicted_3d_pos_flip\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "            losses_3d_valid.append(epoch_loss_3d_valid / N)\n",
    "\n",
    "    elapsed = (time() - start_time) / 60\n",
    "\n",
    "    if no_eval:\n",
    "        print('[%d] time %.2f lr %f 3d_train %f' % (\n",
    "            epoch + 1,\n",
    "            elapsed,\n",
    "            lr,\n",
    "            losses_3d_train[-1] * 1000))\n",
    "    else:\n",
    "        print('[%d] time %.2f lr %f 3d_train %f 3d_valid %f' % (\n",
    "            epoch + 1,\n",
    "            elapsed,\n",
    "            lr,\n",
    "            losses_3d_train[-1] * 1000,\n",
    "            losses_3d_valid[-1] * 1000))\n",
    "\n",
    "    # Decay learning rate exponentially\n",
    "    lr *= lr_decay\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] *= lr_decay\n",
    "    epoch += 1\n",
    "\n",
    "    # Save training curves after every epoch, as .png images (if requested)\n",
    "    if epoch > 3:\n",
    "\n",
    "        plt.figure()\n",
    "        epoch_x = np.arange(3, len(losses_3d_train)) + 1\n",
    "        plt.plot(epoch_x, losses_3d_train[3:], '--', color='C0')\n",
    "#         plt.plot(epoch_x, losses_3d_train_eval[3:], color='C0')\n",
    "        plt.plot(epoch_x, losses_3d_valid[3:], color='C1')\n",
    "        plt.legend(['3d train', '3d train (eval)', '3d valid (eval)'])\n",
    "        plt.ylabel('MPJPE (m)')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.xlim((3, epoch))\n",
    "        plt.savefig(os.path.join('loss_curves', 'loss_3d.png'))\n",
    "        plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f0d38e9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/projectnb/cs585/bargav25/project/PoseFormerV2-main'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911bf415",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9a721075",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = 'checkpoint/simpletrans_model'\n",
    "torch.save(model_pos.state_dict(), ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf6c997",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
